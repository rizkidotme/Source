{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python crawler/crawler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: bludsearch [OPTIONS] [FILENAME]...\n",
      "\n",
      "Options:\n",
      "  --model [openai|minilm|bludescale|sgpt|sgpt-1.3B]\n",
      "                                  Preset model to use for embedding  [default:\n",
      "                                  bludescale]\n",
      "  --encoding TEXT                 Encoding to use for reading text files\n",
      "                                  [default: utf-8]\n",
      "  --transformer-model TEXT        Custom Huggingface transformers model name\n",
      "                                  to use for embedding\n",
      "  --windows TEXT                  Embedding windows to extract. A comma-\n",
      "                                  separated list of the format\n",
      "                                  \"size[_offset=0][_rewind=0]. A window with\n",
      "                                  size 128, offset 0, and rewind of 16\n",
      "                                  (128_0_16) will embed the document in chunks\n",
      "                                  of 128 tokens which partially overlap by 16.\n",
      "                                  Only the first window is used for search.\n",
      "                                  [default: 128_0_16]\n",
      "  --no-server                     Do not start the UI server (only process)\n",
      "  --port INTEGER                  Port to use for embedding server  [default:\n",
      "                                  8080]\n",
      "  --host TEXT                     Host to use for embedding server  [default:\n",
      "                                  0.0.0.0]\n",
      "  --pool-size INTEGER             Max number of embedding tokens to pool\n",
      "                                  together in requests\n",
      "  --pool-count INTEGER            Max number of embeddings to pool together in\n",
      "                                  requests\n",
      "  --doc-token-pre TEXT            Token to prepend to each document in\n",
      "                                  transformer models (default: None)\n",
      "  --doc-token-post TEXT           Token to append to each document in\n",
      "                                  transformer models (default: None)\n",
      "  --query-token-pre TEXT          Token to prepend to each query in\n",
      "                                  transformer models (default: None)\n",
      "  --query-token-post TEXT         Token to append to each query in transformer\n",
      "                                  models (default: None)\n",
      "  --num-results INTEGER           Number of results (neighbors) to retrieve\n",
      "                                  per file for queries  [default: 10]\n",
      "  --annoy                         Use approximate kNN via Annoy for queries\n",
      "                                  (faster querying at a slight cost of\n",
      "                                  accuracy); if false, use exact exhaustive\n",
      "                                  kNN  [default: True]\n",
      "  --num-annoy-trees INTEGER       Number of trees to use for approximate kNN\n",
      "                                  via Annoy  [default: 100]\n",
      "  --svm                           Use SVM instead of any kind of kNN for\n",
      "                                  queries (slower and only works on symmetric\n",
      "                                  models)\n",
      "  --svm-c FLOAT                   SVM regularization parameter; higher values\n",
      "                                  penalize mispredictions more  [default: 1.0]\n",
      "  --explain-split-count INTEGER   Number of splits on a given window to use\n",
      "                                  for explaining a query  [default: 9]\n",
      "  --explain-split-divide INTEGER  Factor to divide the window size by to get\n",
      "                                  each split length for explaining a query\n",
      "                                  [default: 6]\n",
      "  --num-explain-highlights INTEGER\n",
      "                                  Number of split results to highlight for\n",
      "                                  explaining a query  [default: 2]\n",
      "  --force                         Force process even if cached\n",
      "  --silent                        Do not print progress information\n",
      "  --no-confirm                    Do not show cost and ask for confirmation\n",
      "                                  before processing with OpenAI\n",
      "  --version                       Print version and exit\n",
      "  --list-models                   List preset models and exit\n",
      "  --show-bludsearch-dir           Print the directory bludsearch will use to\n",
      "                                  store processed files and exit\n",
      "  --bludsearch-dir PATH           Directory to store bludsearch files in\n",
      "  --help                          Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!bludsearch ./crawler/*txt "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
