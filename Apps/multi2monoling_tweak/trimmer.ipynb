{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vocabtrimmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/rizki/blud/hf/ori and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import vocabtrimmer\n",
    "\n",
    "trimmer = vocabtrimmer.VocabTrimmer(model_name=\"intfloat/multilingual-e5-small\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pada model ini 81 % cuma untuk weight pada word embedding embeddings.word_embeddings.weight\t\n",
    "untuk itu kita bisa mengurangi dari 250k tokenizer jadi 30k dengan begitu total params akan 70 %  agar lebih mudah di jalankan pada inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rizki/.local/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rizki/blud/tweak/trimmer.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B34.126.64.209/home/rizki/blud/tweak/trimmer.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trimmer\u001b[39m.\u001b[39;49mtrim_vocab(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.126.64.209/home/rizki/blud/tweak/trimmer.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     language\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.126.64.209/home/rizki/blud/tweak/trimmer.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     path_to_save\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/home/rizki/blud/hf/id\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.126.64.209/home/rizki/blud/tweak/trimmer.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     target_vocab_size\u001b[39m=\u001b[39;49m\u001b[39m50000\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.126.64.209/home/rizki/blud/tweak/trimmer.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/vocabtrimmer/base_trimmer.py:171\u001b[0m, in \u001b[0;36mVocabTrimmer.trim_vocab\u001b[0;34m(self, language, path_to_save, dataset, dataset_column, dataset_name, dataset_split, tokens_to_keep, target_vocab_size, min_frequency, chunk, cache_file_vocab, cache_file_frequency, overwrite)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m# vocab mining\u001b[39;00m\n\u001b[1;32m    170\u001b[0m dataset_name \u001b[39m=\u001b[39m language \u001b[39mif\u001b[39;00m dataset \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mmc4\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvocabtrimmer/mc4_validation\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m dataset_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m dataset_name\n\u001b[0;32m--> 171\u001b[0m new_vocab \u001b[39m=\u001b[39m vocab_miner(\n\u001b[1;32m    172\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name,\n\u001b[1;32m    173\u001b[0m     language\u001b[39m=\u001b[39;49mlanguage,\n\u001b[1;32m    174\u001b[0m     dataset\u001b[39m=\u001b[39;49mdataset,\n\u001b[1;32m    175\u001b[0m     dataset_column\u001b[39m=\u001b[39;49mdataset_column,\n\u001b[1;32m    176\u001b[0m     dataset_name\u001b[39m=\u001b[39;49mdataset_name,\n\u001b[1;32m    177\u001b[0m     dataset_split\u001b[39m=\u001b[39;49mdataset_split,\n\u001b[1;32m    178\u001b[0m     target_vocab_size\u001b[39m=\u001b[39;49mtarget_vocab_size,\n\u001b[1;32m    179\u001b[0m     min_frequency\u001b[39m=\u001b[39;49mmin_frequency,\n\u001b[1;32m    180\u001b[0m     chunk\u001b[39m=\u001b[39;49mchunk,\n\u001b[1;32m    181\u001b[0m     cache_file_frequency\u001b[39m=\u001b[39;49mcache_file_frequency,\n\u001b[1;32m    182\u001b[0m     cache_file_vocab\u001b[39m=\u001b[39;49mcache_file_vocab,\n\u001b[1;32m    183\u001b[0m     overwrite\u001b[39m=\u001b[39;49moverwrite\n\u001b[1;32m    184\u001b[0m )\n\u001b[1;32m    186\u001b[0m vocab \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mall_special_tokens, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mall_special_ids))\n\u001b[1;32m    187\u001b[0m vocab\u001b[39m.\u001b[39mupdate(new_vocab)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/vocabtrimmer/vocab_miner.py:72\u001b[0m, in \u001b[0;36mvocab_miner\u001b[0;34m(model, language, dataset, dataset_column, dataset_name, dataset_split, target_vocab_size, min_frequency, chunk, cache_file_vocab, cache_file_frequency, overwrite)\u001b[0m\n\u001b[1;32m     70\u001b[0m # tokenization\n\u001b[1;32m     71\u001b[0m logging.info(f\"caching all tokens to {cache_file_frequency}\")\n\u001b[0;32m---> 72\u001b[0m batch = []\n\u001b[1;32m     73\u001b[0m fq = defaultdict(int)\n\u001b[1;32m     74\u001b[0m for t in tqdm(dataset):\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "trimmer.trim_vocab(\n",
    "    language=\"id\",\n",
    "    path_to_save=\"/home/rizki/blud/hf/id\",\n",
    "    target_vocab_size=30000,\n",
    "    overwrite=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
